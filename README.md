这是一个 [PyTorch](https://pytorch.org/) 的项目: a PyTorch Tutorial to Transfer Learning

这是 [a series of pytorch projects](https://github.com/L1aoXingyu/a-series-of-pytorch-projects) 中的第二个项目，在这个项目中我们会学习到如何使用卷积神经网络和循环神经网络实现给图片加字幕的效果。

需要大家了解 PyTorch 的基本知识，同时要掌握卷积神经网络和循环神经网络的知识。

项目使用 `PyTorch 1.0` 和 `python3.7`

**项目要求**:  
1. 阅读 [Show, Attend, and Tell](https://arxiv.org/abs/1502.03044) 这篇论文
2. 找到代码中 `TODO` 部分，完成代码的内容，并训练网络 (`dataset.py` 和 `train.py`)
3. （可选）根据项目代码，自己从头写一个相似的项目作为练习



# 目录

[**Objective**](https://github.com/tensorinfinitysip/a-PyTorch-Project-to-Image-Caption#objective)

[**Concepts**](https://github.com/tensorinfinitysip/a-PyTorch-Project-to-Image-Caption#concepts)

[**Overview**](https://github.com/tensorinfinitysip/a-PyTorch-Project-to-Image-Caption#overview)

[**Implementation**](https://github.com/tensorinfinitysip/a-PyTorch-Project-to-Image-Caption#implementation)



# Objective

**在这个项目中，我们将使用 CNN 和 RNN 建立一个模型，这个模型可以对一张给定的图片，生成一个该图片的描述字幕。**

为了使整个项目简单且易于实现和教学，我们主要以 [Show, Attend, and Tell](https://arxiv.org/abs/1502.03044) 这篇论文为基础进行算法的实现，所以首先请大家阅读以下这篇论文，虽然这篇论文并不是目前的 state-of-the-art，但是在当时这篇论文仍然引起了非常大的关注，值得我们去学习和实现。作者的原始实现能够从[这里找到](https://github.com/kelvinxu/arctic-captions)。

通过实现论文中的方法，我们可以学会如何使用 CNN 来提取图片特征，同时利用 RNN 来生成文字序列，同时论文中的方法还利用了**注意力机制**，这个机制能够帮助模型学习在生成每个单词的时候，模型应该更加注意图片中的哪些区域。

下面是一些图片和相关生成的描述的例子，通过此项目的练习，最后我们可以达到相似的效果：



<img src='assets/ex1.jpg' width='800'>



<img src='assets/ex2.jpg' width='800'>





# Concepts

- **Image captioning**. 这个概念上面已经提及了，就是对一张给定的图片，模型会生成图片相关的描述。 

- **编码器和解码器的结构**. 编码器和解码器我们已经在课程里面学习过了，具体来讲就是通过编码器将输入编码成一个固定形式的 code，然后通过解码器再将改 code 重新解码成需要的形式，这里就是通过 RNN 解码成一个一个的单词，将这些单词组成序列构成具有上下文关系的句子。
- **注意力机制**. 注意力机制目前在深度学习中已经得到了广泛的应用，不管是计算机视觉还是自然语言处理，效果比较好，同时实现也不复杂。除此之外，注意力机制也非常符合人脑的机制，我们在观察任何东西的时候，都会注意到其中重要的部分，这有利于我们抓住重点而忽略一些不重要的部分。在模型中，注意力机制帮助我们的生成每个单词的时候，知道图像中哪些像素更加重要，应该更加关注图片中的哪些位置，这有利于我们生成正确的描述句子。
- **迁移学习**. 这个部分在[该项目](https://github.com/tensorinfinitysip/a-PyTorch-Project-to-Transfer-Learning)中有详细的介绍。



# Overview

在这个部分，我们会根据论文来讲解模型中的每个部分，如果你对这个部分非常熟悉，可以直接跳到[实现](https://github.com/tensorinfinitysip/a-PyTorch-Project-to-Image-Caption#implementation)的部分来完成代码。



## Encoder

在 image caption 中，编码器主要的作用是将一张输入的3通道的图片编码成一个固定格式的 code，这个 code 可以作为原始图片的一个特征表达。

因为是对图片进行编码，当然 CNN 是最好的选择，目前有很多成熟的 CNN 结构可以供我们选择，比如 ResNet，ResNext 等等。

我们需要从头训练编码器吗？答案是否定的，如果你还记得迁移学习的项目，你就知道我们并不需要重新训练，可以使用在 ImageNet 上面预训练的模型，这些预训练的模型在提取图片特征上具有很好的效果。

在这个项目中，为了缩短训练时间，我们选择在 ImageNet 上预训练的50层的 ResNet，我们只需要微调这个部分的参数来进一步提升模型的性能或者是固定这个部分的参数不变，这就是我们之前学习到的迁移学习。 当然我们也鼓励你使用别的预训练模型，比如论文中使用的 VGG，当然使用的模型必须要进行修改，我们会扔掉模型最后的全连接层，因为这些全连接层主要是用来完成分类任务的。

<img src='assets/encoder.png' width='700'>

这个模型会逐步地对图片提取特征，通过逐渐的卷积和池化操作，图片特征会变得越来越小，通道越来越多，此时提取的特征表达具有语义信息，最后提取的特征一共有2048的维度，大小是 14 x 14，即最后的特征形状是 `2048, 14, 14`。



# Decoder

解码器的主要作用就是**通过编码之后的图片，一步一步来生成一句图像描述**。

因为要生成一段序列，回顾我们学习的知识，使用循环神经网络是一个非常好的选择，这里我们使用 LSTM。

我们首先描述一下不加注意力机制的算法流程，这样比较简单清晰，同时易于理解算法逻辑。首先我们将编码之后的特征做一个全局平均池化，然后通过仿射变换之后作为隐含状态 $h_0$ 输入到 LSTM 当中，然后可以生成一个单词，同时生成下一步的隐含状态 $h_1$，接着该隐含状态和当前预测的单词作为下一次的输入，再一次输入到 LSTM 当中得到下一步的输出，通过不断的生成，直到最后模型输出一个结束标志 `<end>`，我们就终止模型的继续生成。

<img src='assets/decoder.png' width='800'>

如果使用注意力机制，在序列中生成每一个单词的时候，**模型需要学会每一个单词需要注意图片中的哪一个位置**。例如在生成 `man` 这个单词的时候，模型需要注意到图片中有人的区域。所以加入注意力机制之后，解码器不再是简单的对特征做全局平均，而是先通过一个注意力矩阵和特征相乘，能够让模型知道应该更加注意哪些像素点，然后再输入到解码器里面生成当前的单词。

<img src='assets/decoder_att.png' width='800'>

# Attention

上面我们讲了在每一步预测的时候，模型需要使用注意力矩阵和特征相乘，注意力机制就是用了计算这个注意力矩阵的。

我们来思考一下注意力机制到底是如何起作用的，这有助于我们理解算法中的注意力机制是如何计算的。对于一张图片，我们看到之后需要对图片进行描述，当我们描述到某一个单词的时候，比如 `a man` ，那么我们会注意到图片中人的区域，所以这个时候我们更加注意有人区域的像素点，而当描述到 `carriage` 的时候，我们又会注意到有马车的区域，所以每一次当我们要注意某一个区域的时候，我们需要考虑我们的句子描述到了哪里。

上面我们用了一个例子来说明注意力机制是如何起作用的，即每一次我们需要注意图片中那个位置，我们需要通过当前的序列信息来帮助我们，如果用算法来讲，每一次计算注意力矩阵的时候，都需要用 LSTM 的隐含状态，因为这个隐含状态保留了之前句子的信息。

<img src='assets/att.png' width='700'>

这里我们使用 soft attention，也就是我们需要每一步的注意力矩阵所有的元素求和为1，比如在 t 时间步，那么
$$
\sum_{i} A_{i, t} = 1
$$
我们可以使用 `softmax` 来轻松的实现这样一个操作，同时这个也可以作为一个概率的解释，再生成下一个单词的时候，哪个部分被注意到的概率最大。



# Overall

我们将前面讲的所有内容合并在一起，就是整体的网络架构，合并在一起看会更加清晰。



<img src='assets/overall.png' width='800'>

下面我们描述一下整体的算法流程：

1. 首先对于一张3通道的输入图片，我们通过编码器得到一个高维的特征表达，然后将这个高维特征表达线性变换为 LSTM 初始化的隐含状态
2. 在每一个时间步
   - 编码的特征表达和前一步的隐含状态一起输入到注意力网络中，计算得到注意力矩阵，然后将编码的特征和注意力矩阵进行加权平均，帮助模型学习到哪些像素点应该更加注意
   - 前一步生成的单词和加权平均的特征表达一起输入到解码器中，生成当前步的输出单词



# Implementation

